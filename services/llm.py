import json
import os
from pathlib import Path
from typing import Dict, Optional

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

from config import settings
from services.log import logger

class LLMService:
    def __init__(self):
        # 1. åˆå§‹åŒ– Client (æ”¯æ´ Groq èˆ‡ OpenAI)
        api_key = settings.OPENAI_API_KEY
        base_url = settings.OPENAI_BASE_URL
        
        if OpenAI and api_key:
            logger.info(f"ğŸ§  LLM Service init with model: {settings.LLM_MODEL}")
            self.client = OpenAI(
                api_key=api_key,
                base_url=base_url
            )
            self.enabled = True
        else:
            self.client = None
            self.enabled = False
            logger.warning("âš ï¸ LLM æœªå•Ÿç”¨ï¼Œå°‡ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼ã€‚")

        # 2. è¼‰å…¥éœæ…‹è³‡æº
        self.prompts_path = Path("assets/prompts/system_prompts.json")
        self.corpus_path = Path("assets/knowledge/rag_corpus.json")
        
        self.system_prompts = self._load_json(self.prompts_path)
        self.rag_corpus = self._load_json(self.corpus_path)

    def _load_json(self, path: Path) -> Dict:
        if not path.exists():
            return {}
        try:
            with open(path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"è®€å– {path} å¤±æ•—: {e}")
            return {}

    def get_system_prompt(self, persona: str = "doctor") -> str:
        base = self.system_prompts.get("common_rules", "ä½ æ˜¯ä¸€å€‹çœ¼ç§‘åŠ©æ‰‹ã€‚")
        roles = self.system_prompts.get("roles", {})
        role_data = roles.get(persona, roles.get("doctor", {}))
        return f"{base}\n\n{role_data.get('prompt', '')}"
    
    def get_task_prompt(self, task_name: str, **kwargs) -> str:
        """
        å–å¾—ä»»å‹™å‹ Prompt ä¸¦å¡«å…¥è®Šæ•¸
        ä½¿ç”¨æ–¹å¼: get_task_prompt("questionnaire_summary", survey_id="ç™½å…§éšœ", answers_str="...")
        """
        tasks = self.system_prompts.get("tasks", {})
        raw_prompt = tasks.get(task_name, "")
        
        # è‡ªå‹•æ›¿æ›è®Šæ•¸ (å¦‚ {survey_id})
        try:
            return raw_prompt.format(**kwargs)
        except KeyError as e:
            logger.error(f"Prompt è®Šæ•¸ç¼ºå¤±: {e}")
            return raw_prompt # å›å‚³æœªæ›¿æ›çš„å­—ä¸²ä»¥å…å ±éŒ¯

    def get_knowledge_context(self, keyword: str) -> str:
        """ç°¡æ˜“ RAG æª¢ç´¢"""
        if not keyword: return ""
        hits = [v for k, v in self.rag_corpus.items() if keyword in k or keyword in v[:20]]
        return "ã€åƒè€ƒé†«å­¸è³‡æ–™ã€‘\n" + "\n".join(hits[:2]) if hits else ""

    def generate_response(self, user_text: str, persona: str = "doctor", context_keyword: Optional[str] = None) -> str:
        if not self.enabled:
            return f"[ç³»çµ±æ¨¡æ“¬ ({settings.LLM_MODEL})]: {user_text}"

        try:
            system_msg = self.get_system_prompt(persona)
            if context_keyword:
                system_msg += f"\n\n{self.get_knowledge_context(context_keyword)}"

            response = self.client.chat.completions.create(
                model=settings.LLM_MODEL, # ä½¿ç”¨ Config ä¸­çš„æ¨¡å‹åç¨±
                messages=[
                    {"role": "system", "content": system_msg},
                    {"role": "user", "content": user_text}
                ],
                temperature=0.7,
                max_tokens=500
            )
            return response.choices[0].message.content.strip()

        except Exception as e:
            logger.error(f"LLM Error: {e}")
            return "æŠ±æ­‰ï¼ŒAI æœå‹™æš«æ™‚ç„¡æ³•é€£ç·šã€‚"

llm_service = LLMService()